wva:
  enabled: true
  imagePullPolicy: Always
  metrics:
    enabled: true
    port: 8443
    secure: true

  reconcileInterval: 60s
  prometheus:
    monitoringNamespace: llm-d-monitoring  # Namespace for Prometheus monitoring
    serviceAccountName: "kube-prometheus-stack-prometheus"
    baseURL: "https://llmd-kube-prometheus-stack-prometheus.llm-d-monitoring.svc.cluster.local:9090"
    # Development security configuration (relaxed for easier development)
    tls:
      insecureSkipVerify: true   # Development: true, Production: false
      caCertPath: ""  # Empty string to disable CA cert when using insecureSkipVerify
      # caCert: |  # Uncomment and provide your CA certificate
      #   -----BEGIN CERTIFICATE-----
      #   YOUR_CA_CERTIFICATE_HERE
      #   -----END CERTIFICATE-----

  # Environment variable to enable experimental hybrid-based optimization
  #  When "on", runs both capacity analyzer and model-based optimizer with arbitration
  #  When "model-only" runs model-based optimizer only
  #  When "off" or unset, runs capacity analyzer only (default, reactive mode)
  experimentalHybridOptimization: false  # Enable experimental hybrid optimization (default: false)
  scaleToZero: false  # Enable scaling variants to zero replicas (default: false)

llmd:
  # Namespace where the llm-d inference-scheduling stack is deployed
  # For wva-only mode: Set this to your existing inference-scheduling namespace (default auto-detected from LLMD_NAMESPACE env var)
  # For full installation: This will be set automatically to match the deployment namespace
  namespace: llm-d-autoscaler
  # Model service name (Service name of the vLLM inference pods)
  # For wva-only mode: Auto-detected as ms-{LLMD_RELEASE_NAME_POSTFIX}-llm-d-modelservice, but can be explicitly set here
  # For full installation: Auto-generated from release name postfix, but can be overridden here
  modelName: ms-workload-autoscaler-llm-d-modelservice
  # Model ID must match the model configured in your inference-scheduling deployment
  modelID: "Qwen/Qwen3-0.6B"

va:
  enabled: true
  accelerator: H100
  sloTpot: 10
  sloTtft: 1000

hpa:
  enabled: true
  maxReplicas: 10
  targetAverageValue: "1"

vllmService:
  enabled: false
  nodePort: 30000
  interval: 15s
  scheme: http  # vLLM emulator runs on HTTP
