multinode: false

modelArtifacts:
  uri: "hf://random"
  name: random
  size: 5Mi
  labels:
    llm-d.ai/inference-serving: "true"
    llm-d.ai/guide: "simulated-accelerators"
    llm-d.ai/accelerator-variant: "cpu"
    llm-d.ai/model: "random"

routing:
  servicePort: 8000
  proxy:
    image: ghcr.io/llm-d/llm-d-routing-sidecar:v0.5.0
    connector: nixlv2
    secure: false

decode:
  create: true
  replicas: 3
  monitoring:
    podmonitor:
      enabled: true
      portName: "vllm"  # decode vLLM service port (from routing.proxy.targetPort)
      path: "/metrics"
      interval: "30s"
  containers:
    - name: "vllm"
      image: "ghcr.io/llm-d/llm-d-inference-sim:v0.7.1"
      modelCommand: imageDefault
      ports:
        - containerPort: 8200
          name: vllm
          protocol: TCP
      mountModelVolume: true
      volumeMounts:
      - name: metrics-volume
        mountPath: /.config
      startupProbe:
        httpGet:
          path: /v1/models
          port: vllm
        initialDelaySeconds: 15
        periodSeconds: 30
        timeoutSeconds: 5
        failureThreshold: 60
      livenessProbe:
        httpGet:
          path: /health
          port: vllm
        periodSeconds: 10
        timeoutSeconds: 5
        failureThreshold: 3
      readinessProbe:
        httpGet:
          path: /v1/models
          port: vllm
        periodSeconds: 5
        timeoutSeconds: 2
        failureThreshold: 3
  volumes:
    - name: metrics-volume
      emptyDir: {}

prefill:
  create: true
  replicas: 1
  monitoring:
    podmonitor:
      enabled: true
      portName: "vllm"  # prefill vLLM service port (from routing.servicePort)
      path: "/metrics"
      interval: "30s"
  containers:
    - name: "vllm"
      image: "ghcr.io/llm-d/llm-d-inference-sim:v0.7.1"
      modelCommand: imageDefault
      ports:
        - containerPort: 8000
          name: vllm
          protocol: TCP
      mountModelVolume: true
      volumeMounts:
      - name: metrics-volume
        mountPath: /.config
      startupProbe:
        httpGet:
          path: /v1/models
          port: vllm
        initialDelaySeconds: 15
        periodSeconds: 30
        timeoutSeconds: 5
        failureThreshold: 60
      livenessProbe:
        httpGet:
          path: /health
          port: vllm
        periodSeconds: 10
        timeoutSeconds: 5
        failureThreshold: 3
      readinessProbe:
        httpGet:
          path: /v1/models
          port: vllm
        periodSeconds: 5
        timeoutSeconds: 2
  volumes:
    - name: metrics-volume
      emptyDir: {}
