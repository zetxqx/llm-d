apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ../base
patches:
  - target:
      kind: LeaderWorkerSet
      name: wide-ep-llm-d-prefill
    patch: |-
      # Prefer the same block and subblock.
      # https://github.com/llm-d/llm-d/tree/main/docs/infra-providers/gke#configuring-support-for-rdma-on-gke-workload-pods
      - op: add
        path: /spec/leaderWorkerTemplate/workerTemplate/spec/affinity
        value:
          podAffinity:
            # Subblock affinity cannot guarantee all pods in the replica
            # are in the same subblock, but is better than random spreading
            preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 2
              podAffinityTerm:
                labelSelector:
                  matchLabels:
                    llm-d.ai/role: prefill
                matchLabelKeys:
                - component
                topologyKey: cloud.google.com/gce-topology-block
            - weight: 1
              podAffinityTerm:
                labelSelector:
                  matchLabels:
                    llm-d.ai/role: prefill
                matchLabelKeys:
                - component
                topologyKey: cloud.google.com/gce-topology-subblock
  - target:
      kind: LeaderWorkerSet
      name: wide-ep-llm-d-decode
    patch: |-
      # Prefer the same block and subblock.
      # https://github.com/llm-d/llm-d/tree/main/docs/infra-providers/gke#configuring-support-for-rdma-on-gke-workload-pods
      - op: add
        path: /spec/leaderWorkerTemplate/workerTemplate/spec/affinity
        value:
          podAffinity:
            # Subblock affinity cannot guarantee all pods in the replica
            # are in the same subblock, but is better than random spreading
            preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 2
              podAffinityTerm:
                labelSelector:
                  matchLabels:
                    llm-d.ai/role: decode
                matchLabelKeys:
                - component
                topologyKey: cloud.google.com/gce-topology-block
            - weight: 1
              podAffinityTerm:
                labelSelector:
                  matchLabels:
                    llm-d.ai/role: decode
                matchLabelKeys:
                - component
                topologyKey: cloud.google.com/gce-topology-subblock
  # Common configurations for prefill and decode.
  - target:
      kind: LeaderWorkerSet
    patch: |-
      # Enable privileged container on GKE to perform GPU initiated RDMA.
      # See https://github.com/llm-d/llm-d/tree/main/docs/infra-providers/gke#configuring-support-for-rdma-on-gke-workload-pods.
      - op: add
        path: /spec/leaderWorkerTemplate/workerTemplate/spec/containers/0/securityContext/privileged
        value: true
      # Add GKE specific RDMA configuration.
      - op: add
        path: /spec/leaderWorkerTemplate/workerTemplate/metadata/annotations
        value:
          networking.gke.io/default-interface: 'eth0'
          networking.gke.io/interfaces: |
            [
              {"interfaceName":"eth0","network":"default"},
              {"interfaceName":"eth2","network":"rdma-0"},
              {"interfaceName":"eth3","network":"rdma-1"},
              {"interfaceName":"eth4","network":"rdma-2"},
              {"interfaceName":"eth5","network":"rdma-3"},
              {"interfaceName":"eth6","network":"rdma-4"},
              {"interfaceName":"eth7","network":"rdma-5"},
              {"interfaceName":"eth8","network":"rdma-6"},
              {"interfaceName":"eth9","network":"rdma-7"}
            ]
      # Add GKE gib per https://cloud.google.com/ai-hypercomputer/docs/create/gke-ai-hypercompute-custom#configure-pod-manifests-rdma
      - op: add
        path: /spec/leaderWorkerTemplate/workerTemplate/spec/volumes/-
        value:
          name: gib
          hostPath:
            path: /home/kubernetes/bin/gib
            type: ""
      - op: add
        path: /spec/leaderWorkerTemplate/workerTemplate/spec/containers/0/volumeMounts/-
        value:
          mountPath: /usr/local/gib
          name: gib
      # Env vars 
      - op: add
        path: /spec/leaderWorkerTemplate/workerTemplate/spec/containers/0/env/-
        value:
          # GCP pairs GPU NICs to PCIe bridges (https://cloud.google.com/compute/docs/gpus/gpu-network-bandwidth#h200-gpus),
          # which can lead to NVSHMEM's automatic distance assignment algorithm selecting NICs
          # inefficiently. Instead, instruct NVSHMEM to select the NIC that aligns to the
          # index of the GPU on the node. Relies on https://github.com/deepseek-ai/DeepEP/pull/466
          name: DEEP_EP_DEVICE_TO_HCA_MAPPING
          value: "0:mlx5_0:1,1:mlx5_1:1,2:mlx5_2:1,3:mlx5_3:1,4:mlx5_4:1,5:mlx5_5:1,6:mlx5_6:1,7:mlx5_7:1"
      # Source nccl env at startup.
      # https://cloud.google.com/ai-hypercomputer/docs/create/gke-ai-hypercompute-custom#configure-pod-manifests-rdma
      - op: add
        path: /spec/leaderWorkerTemplate/workerTemplate/spec/containers/0/env/-
        value:
          name: BASH_ENV
          value: "/usr/local/gib/scripts/set_nccl_env.sh"
      # This is recommended on gke.
      # https://github.com/llm-d/llm-d/tree/main/docs/infra-providers/gke#configuring-support-for-rdma-on-gke-workload-pods
      - op: add
        path: /spec/leaderWorkerTemplate/workerTemplate/spec/containers/0/env/-
        value:
          name: NVSHMEM_DISABLED_GDRCOPY
          value: "true"
      # We add the volume for different providers since the hostPath can be different.
      - op: replace
        path: /spec/leaderWorkerTemplate/workerTemplate/spec/volumes/1
        value:
          name: hf-cache
          hostPath:
            path: /mnt/stateful_partition/kube-ephemeral-ssd/shared_disk/vllm-hf-cache/
            type: DirectoryOrCreate
      - op: replace
        path: /spec/leaderWorkerTemplate/workerTemplate/spec/volumes/2
        value:
          name: jit-cache
          hostPath:
            path: /mnt/stateful_partition/kube-ephemeral-ssd/shared_disk/vllm-jit-cache/
            type: DirectoryOrCreate
