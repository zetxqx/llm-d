---
# Source: inferencepool/templates/rbac.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: gaie-inference-scheduling-epp
  namespace: llmd-standalone
  labels:
    app.kubernetes.io/name: gaie-inference-scheduling-epp
    app.kubernetes.io/version: "v1.2.1"
---
# Source: inferencepool/templates/epp-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: gaie-inference-scheduling-epp
  namespace: llmd-standalone
data:
  default-plugins.yaml: |
    apiVersion: inference.networking.x-k8s.io/v1alpha1
    kind: EndpointPickerConfig
    plugins:
    - type: queue-scorer
    - type: kv-cache-utilization-scorer
    - type: prefix-cache-scorer
    schedulingProfiles:
    - name: default
      plugins:
      - pluginRef: queue-scorer
        weight: 2
      - pluginRef: kv-cache-utilization-scorer
        weight: 2
      - pluginRef: prefix-cache-scorer
        weight: 3
---
# Source: inferencepool/templates/epp-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: envoy
  namespace: llmd-standalone
data:
  envoy.yaml: |
    admin:
      address:
        socket_address:
          address: 127.0.0.1
          port_value: 19000
      access_log:
        - name: envoy.access_loggers.file
          typed_config:
            "@type": type.googleapis.com/envoy.extensions.access_loggers.file.v3.FileAccessLog
            path: /dev/null
    static_resources:
      listeners:
        - name: envoy-proxy-ready-0.0.0.0-19001
          address:
            socket_address:
              address: 0.0.0.0
              port_value: 19001
          filter_chains:
            - filters:
                - name: envoy.filters.network.http_connection_manager
                  typed_config:
                    "@type": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager
                    stat_prefix: envoy-ready-http
                    route_config:
                      name: local_route
                      virtual_hosts:
                        - name: prometheus_stats
                          domains: ["*"]
                          routes:
                            - match:
                                prefix: "/stats/prometheus"
                              route:
                                cluster: "prometheus_stats"
                    http_filters:
                      - name: envoy.filters.http.health_check
                        typed_config:
                          "@type": type.googleapis.com/envoy.extensions.filters.http.health_check.v3.HealthCheck
                          pass_through_mode: false
                          headers:
                            - name: ":path"
                              string_match:
                                exact: "/ready"
                      - name: envoy.filters.http.router
                        typed_config:
                          "@type": type.googleapis.com/envoy.extensions.filters.http.router.v3.Router
        - name: vllm
          address:
            socket_address:
              address: 0.0.0.0
              port_value: 8081
          per_connection_buffer_limit_bytes: 32768
          access_log:
            - name: envoy.access_loggers.file
              filter:
                response_flag_filter:
                  flags: ["NR"]
              typed_config:
                "@type": type.googleapis.com/envoy.extensions.access_loggers.file.v3.FileAccessLog
                path: /dev/stdout
                log_format:
                  text_format_source:
                    inline_string: "{\"start_time\":\"%START_TIME%\",\"method\":\"%REQ(:METHOD)%\",...}\n"
          filter_chains:
            - name: vllm
              filters:
                - name: envoy.filters.network.http_connection_manager
                  typed_config:
                    "@type": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager
                    stat_prefix: http-8081
                    route_config:
                      name: vllm
                      virtual_hosts:
                        - name: vllm-default
                          domains: ["*"]
                          routes:
                            - match:
                                prefix: "/"
                              route:
                                cluster: original_destination_cluster
                                timeout: 86400s
                                idle_timeout: 86400s
                                upgrade_configs:
                                  - upgrade_type: websocket
                              typed_per_filter_config:
                                envoy.filters.http.ext_proc:
                                  "@type": type.googleapis.com/envoy.config.route.v3.FilterConfig
                                  config: {}
                    http_filters:
                      - name: envoy.filters.http.ext_proc
                        typed_config:
                          "@type": type.googleapis.com/envoy.extensions.filters.http.ext_proc.v3.ExternalProcessor
                          grpc_service:
                            envoy_grpc:
                              cluster_name: ext_proc
                              authority: localhost:9002
                            timeout: 10s
                          processing_mode:
                            request_header_mode: SEND
                            response_header_mode: SEND
                            request_body_mode: FULL_DUPLEX_STREAMED
                            response_body_mode: FULL_DUPLEX_STREAMED
                            request_trailer_mode: SEND
                            response_trailer_mode: SEND
                          message_timeout: 1000s
                      - name: envoy.filters.http.router
                        typed_config:
                          "@type": type.googleapis.com/envoy.extensions.filters.http.router.v3.Router
                          suppress_envoy_headers: true
                    http2_protocol_options:
                      max_concurrent_streams: 100
                      initial_stream_window_size: 65536
                      initial_connection_window_size: 1048576
                    use_remote_address: true
                    normalize_path: true
                    merge_slashes: true
                    server_header_transformation: PASS_THROUGH
                    common_http_protocol_options:
                      headers_with_underscores_action: REJECT_REQUEST
                    path_with_escaped_slashes_action: UNESCAPE_AND_REDIRECT
                    access_log:
                      - name: envoy.access_loggers.file
                        typed_config:
                          "@type": type.googleapis.com/envoy.extensions.access_loggers.file.v3.FileAccessLog
                          path: /dev/stdout
                          log_format:
                            text_format_source:
                              inline_string: "{\"start_time\":\"%START_TIME%\",\"method\":\"%REQ(:METHOD)%\",...}\n"
      clusters:
        - name: prometheus_stats
          type: STATIC
          connect_timeout: 0.250s
          load_assignment:
            cluster_name: prometheus_stats
            endpoints:
              - lb_endpoints:
                  - endpoint:
                      address:
                        socket_address:
                          address: 127.0.0.1
                          port_value: 19000
        - name: original_destination_cluster
          type: ORIGINAL_DST
          connect_timeout: 1000s
          lb_policy: CLUSTER_PROVIDED
          circuit_breakers:
            thresholds:
              - max_connections: 40000
                max_pending_requests: 40000
                max_requests: 40000
          original_dst_lb_config:
            use_http_header: true
            http_header_name: x-gateway-destination-endpoint
        - name: ext_proc
          type: STATIC
          connect_timeout: 86400s
          lb_policy: LEAST_REQUEST
          circuit_breakers:
            thresholds:
              - max_connections: 40000
                max_pending_requests: 40000
                max_requests: 40000
                max_retries: 1024
          health_checks:
            - timeout: 2s
              interval: 10s
              unhealthy_threshold: 3
              healthy_threshold: 2
              reuse_connection: true
              grpc_health_check:
                service_name: "envoy.service.ext_proc.v3.ExternalProcessor"
              tls_options:
                alpn_protocols: ["h2"]
          transport_socket:
            name: "envoy.transport_sockets.tls"
            typed_config:
              "@type": type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.UpstreamTlsContext
              common_tls_context:
                validation_context:
          typed_extension_protocol_options:
            envoy.extensions.upstreams.http.v3.HttpProtocolOptions:
              "@type": type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions
              explicit_http_config:
                http2_protocol_options:
                  initial_stream_window_size: 65536
                  initial_connection_window_size: 1048576
          load_assignment:
            cluster_name: ext_proc
            endpoints:
              - locality:
                  region: ext_proc/e2e/0
                lb_endpoints:
                  - endpoint:
                      address:
                        socket_address:
                          address: 127.0.0.1
                          port_value: 9002
                    load_balancing_weight: 1
---
# Source: inferencepool/templates/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: gaie-inference-scheduling-epp
  namespace: llmd-standalone
  labels:
    app.kubernetes.io/name: gaie-inference-scheduling-epp
    app.kubernetes.io/version: "v1.2.1"
rules:
- apiGroups: ["inference.networking.x-k8s.io"]
  resources: ["inferenceobjectives"]
  verbs: ["get", "watch", "list"]
- apiGroups: ["inference.networking.k8s.io"]
  resources: ["inferencepools"]
  verbs: ["get", "watch", "list"]
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "watch", "list"]
---
# Source: inferencepool/templates/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: gaie-inference-scheduling-epp
  namespace: llmd-standalone
subjects:
- kind: ServiceAccount
  name: gaie-inference-scheduling-epp
  namespace: llmd-standalone
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: gaie-inference-scheduling-epp
---
# Source: inferencepool/templates/epp-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: gaie-inference-scheduling-epp
  namespace: llmd-standalone
  labels:
    app.kubernetes.io/name: gaie-inference-scheduling-epp
    app.kubernetes.io/version: "v1.2.1"
spec:
  selector:
    inferencepool: gaie-inference-scheduling-epp
  ports:
    - name: grpc-ext-proc
      protocol: TCP
      port: 9002
    - name: http-metrics
      protocol: TCP
      port: 9090
    - name: http
      port: 8081
      protocol: TCP
      targetPort: 8081
  type: ClusterIP
---
# Source: inferencepool/templates/epp-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: gaie-inference-scheduling-epp
  namespace: llmd-standalone
  labels:
    app.kubernetes.io/name: gaie-inference-scheduling-epp
    app.kubernetes.io/version: "v1.2.1"
spec:
  replicas: 1
  strategy:
    # The current recommended EPP deployment pattern is to have a single active replica. This ensures 
    # optimal performance of the stateful operations such prefix cache aware scorer.
    # The Recreate strategy the old replica is killed immediately, and allow the new replica(s) to 
    # quickly take over. This is particularly important in the high availability set up with leader
    # election, as the rolling update strategy would prevent the old leader being killed because 
    # otherwise the maxUnavailable would be 100%.
    type: Recreate
  selector:
    matchLabels:
      inferencepool: gaie-inference-scheduling-epp
  template:
    metadata:
      labels:
        inferencepool: gaie-inference-scheduling-epp
    spec:
      serviceAccountName: gaie-inference-scheduling-epp
      # Conservatively, this timeout should mirror the longest grace period of the pods within the pool
      terminationGracePeriodSeconds: 130
      containers:
      - name: envoy-sidecar
        image: docker.io/envoyproxy/envoy:distroless-v1.33.2
        imagePullPolicy: IfNotPresent
        command:
          - "envoy"
        args:
            - "--service-node"
            - "envoy-sidecar"
            - "--log-level"
            - "trace"
            - "--cpuset-threads"
            - "--drain-strategy"
            - "immediate"
            - "--drain-time-s"
            - "60"
            - "-c"
            - "/etc/envoy/envoy.yaml"
        env:
          - name: NS_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
          - name: POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
        ports:
          - containerPort: 8081
            name: http-8081
          - containerPort: 19001
            name: metrics
        readinessProbe:
          failureThreshold: 1
          httpGet:
            path: /ready
            port: 19001
            scheme: HTTP
          periodSeconds: 5
          successThreshold: 1
          timeoutSeconds: 1
        resources:
          requests:
            cpu: 100m
            memory: 512Mi
        volumeMounts:
          - mountPath: /etc/envoy
            name: config
            readOnly: true
      - name: epp
        image: ghcr.io/llm-d/llm-d-inference-scheduler:v0.4.0
        imagePullPolicy: Always
        args:
        - --pool-name
        - gaie-inference-scheduling
        # The pool namespace is optional because EPP can default to the NAMESPACE env var.
        # We still keep this here so that the template works with older versions of EPP, or other
        # distros of EPP which may not have implemented the NAMESPACE env var defaulting behavior.
        - --pool-namespace
        - llmd-standalone
        - --pool-group
        - "inference.networking.k8s.io"
        - --zap-encoder
        - "json"
        - --config-file
        - "/config/default-plugins.yaml"
        # Pass additional flags via the inferenceExtension.flags field in values.yaml.
        - --v
        - "1"
        - --tracing=false
        - --metrics-endpoint-auth=false
        ports:
        - name: grpc
          containerPort: 9002
        - name: grpc-health
          containerPort: 9003
        - name: metrics
          containerPort: 9090
        livenessProbe:
          grpc:
            port: 9003
            service: inference-extension
          initialDelaySeconds: 5
          periodSeconds: 10
        readinessProbe:
          grpc:
            port: 9003
            service: inference-extension
          periodSeconds: 2
        env:
        - name: NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        volumeMounts:
        - name: plugins-config-volume
          mountPath: "/config"
      volumes:
      - configMap:
          items:
          - key: envoy.yaml
            path: envoy.yaml
          name: envoy
        name: config
      - name: plugins-config-volume
        configMap:
          name: gaie-inference-scheduling-epp
---
# Source: inferencepool/templates/inferencepool.yaml
apiVersion: "inference.networking.k8s.io/v1"
kind: InferencePool
metadata:
  name: gaie-inference-scheduling
  namespace: llmd-standalone
  labels:
    app.kubernetes.io/name: gaie-inference-scheduling-epp
    app.kubernetes.io/version: "v1.2.1"
spec:
  targetPorts:
      - number: 8000
  selector:
    matchLabels:
      llm-d.ai/inferenceServing: "true"
  endpointPickerRef:
    name: gaie-inference-scheduling-epp
    port:
      number: 9002

---
# Source: llm-d-modelservice/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: ms-inference-scheduling-llm-d-modelservice
  labels:
    helm.sh/chart: llm-d-modelservice-v0.3.8
    app.kubernetes.io/version: "v0.3.0"
    app.kubernetes.io/managed-by: Helm
---
# Source: llm-d-modelservice/templates/decode-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ms-inference-scheduling-llm-d-modelservice-decode
  labels:
    helm.sh/chart: llm-d-modelservice-v0.3.8
    app.kubernetes.io/version: "v0.3.0"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 2
  selector:
    matchLabels:
      llm-d.ai/inferenceServing: "true"
      llm-d.ai/model: random_model
      llm-d.ai/role: decode
  template:
    metadata:
      labels:
        llm-d.ai/inferenceServing: "true"
        llm-d.ai/model: random_model
        llm-d.ai/role: decode
    spec:
      initContainers:
        - name: routing-proxy
          args:
            - --port=8000
            - --vllm-port=8200
            - --connector=nixlv2
            - --zap-encoder=json
            - --zap-log-level=debug
            - --secure-proxy=false
          image: ghcr.io/llm-d/llm-d-routing-sidecar:v0.4.0-rc.1
          imagePullPolicy: Always
          ports:
            - containerPort: 8000
          resources: {}
          restartPolicy: Always
          securityContext:
            allowPrivilegeEscalation: false
            runAsNonRoot: true
    
      serviceAccountName: ms-inference-scheduling-llm-d-modelservice
      
      volumes:
        - emptyDir: {}
          name: metrics-volume
        - emptyDir: {}
          name: torch-compile-cache
        - name: model-storage
          emptyDir:
            sizeLimit: 20Gi
        
      containers:
        - name: vllm
          image: ghcr.io/llm-d/llm-d-cuda:v0.3.1
          
          command: ["vllm", "serve"]
          args:
            - Qwen/Qwen3-0.6B
            - --port
            - "8200"
            - --served-model-name
            - "Qwen/Qwen3-0.6B"
            
            - --kv-transfer-config
            - '{"kv_connector":"NixlConnector", "kv_role":"kv_both"}'
            - --disable-uvicorn-access-log
          env:
          - name: CUDA_VISIBLE_DEVICES
            value: "0"
          - name: UCX_TLS
            value: cuda_ipc,cuda_copy,tcp
          - name: VLLM_NIXL_SIDE_CHANNEL_HOST
            valueFrom:
              fieldRef:
                fieldPath: status.podIP
          - name: VLLM_NIXL_SIDE_CHANNEL_PORT
            value: "5557"
          - name: VLLM_LOGGING_LEVEL
            value: INFO
          - name: DP_SIZE
            value: "1"
          - name: TP_SIZE
            value: "1"
          - name: DP_SIZE_LOCAL
            value: "1"
          
          - name: HF_HOME
            value: /model-cache
          - name: HF_TOKEN
            valueFrom:
              secretKeyRef:
                name: llm-d-hf-token
                key: HF_TOKEN
          ports:
          - containerPort: 5557
            protocol: TCP
          - containerPort: 8200
            name: metrics
            protocol: TCP
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8200
            periodSeconds: 10
            timeoutSeconds: 5
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /v1/models
              port: 8200
            periodSeconds: 5
            timeoutSeconds: 2
          startupProbe:
            failureThreshold: 60
            httpGet:
              path: /v1/models
              port: 8200
            initialDelaySeconds: 15
            periodSeconds: 30
            timeoutSeconds: 5
          
          resources:
            limits:
              nvidia.com/gpu: "1"
            requests:
              nvidia.com/gpu: "1"
          
          volumeMounts:
            - mountPath: /.config
              name: metrics-volume
            - mountPath: /.cache
              name: torch-compile-cache
            - name: model-storage
              mountPath: /model-cache

