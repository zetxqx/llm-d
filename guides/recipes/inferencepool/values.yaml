inferencePool:
  apiVersion: "inference.networking.x-k8s.io/v1alpha2"
  modelServers:
    matchLabels:
      "llm-d.ai/inferenceServing": "true"
inferenceExtension:
  monitoring:
    gke:
      enable: false
    prometheus:
      enable: false
