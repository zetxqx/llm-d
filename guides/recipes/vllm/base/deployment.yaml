apiVersion: apps/v1
kind: Deployment
metadata:
  name: llm-d-model-server
spec:
  replicas: 2
  selector:
    matchLabels:
      llm-d.ai/inferenceServing: "true"
  template:
    metadata:
      labels:
        llm-d.ai/inferenceServing: "true"
    spec:
      containers:
        - name: vllm
          image: "ghcr.io/llm-d/llm-d-cuda:v0.3.1"
          imagePullPolicy: IfNotPresent
          command:
            - /bin/bash
            - -c
          args:
            - exec vllm serve --port 8000
          env:
            - name: HUGGING_FACE_HUB_TOKEN
              valueFrom:
                secretKeyRef:
                  name: llm-d-hf-token
                  key: HF_TOKEN
          ports:
            - containerPort: 8000
              name: http
              protocol: TCP
          lifecycle:
            preStop:
              sleep:
                seconds: 30
          livenessProbe:
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            periodSeconds: 1
            successThreshold: 1
            failureThreshold: 5
            timeoutSeconds: 1
          readinessProbe:
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            periodSeconds: 1
            successThreshold: 1
            failureThreshold: 1
            timeoutSeconds: 1
          startupProbe:
            failureThreshold: 600
            initialDelaySeconds: 2
            periodSeconds: 1
            httpGet:
              path: /health
              port: http
              scheme: HTTP
          volumeMounts:
            - mountPath: /data
              name: data
            - mountPath: /dev/shm
              name: shm
      restartPolicy: Always

      enableServiceLinks: false

      terminationGracePeriodSeconds: 130

      volumes:
        - name: data
          emptyDir: {}
        - name: shm
          emptyDir:
            medium: Memory

