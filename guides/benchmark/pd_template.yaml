endpoint:
  stack_name: &stack_name pd-gpt-oss-120b   # user defined name for the stack (results prefix)
  model: &model openai/gpt-oss-120b         # Exact HuggingFace model name. Must match stack deployed.
  namespace: &namespace ${NAMESPACE}        # Namespace where stack is deployed
  base_url: &url http://${GATEWAY_SVC}.${NAMESPACE}.svc.cluster.local:80 # Base URL of inference endpoint  base_url: &url http://${GATEWAY_SVC}.${NAMESPACE}.svc.cluster.local:80  # Base URL of inference endpoint
  hf_token_secret: llm-d-hf-token           # The name of secret that contains the HF token of the stack

control:
  work_dir: $HOME/llm-d-bench-work  # working directory to store temporary and autogenerated files.
                                    # Do not edit content manually.
                                    # If not set, a temp directory will be created.
  kubectl: kubectl                  # kubectl command: kubectl or oc

harness:
  name: &harness_name inference-perf
  results_pvc: ${BENCHMARK_PVC}   # PVC where benchmark results are stored
  namespace: *namespace           # Namespace where harness is deployed. Typically with stack.
  parallelism: 1                  # Number of parallel workload launcher pods to create.
  wait_timeout: 600               # Time (in seconds) to wait for workload launcher pod to complete before terminating.
                                  # Set to 0 to disable timeout.
  image: ghcr.io/llm-d/llm-d-benchmark:v0.4.0
  # dataset_url: &dataset_url none

env:
  - name: RAYON_NUM_THREADS
    value: "4"

workload:                         # yaml configuration for harness workload(s)

  # an example workload using random synthetic data
  sanity_random:
    load:
      type: concurrent
      stages:
      - num_requests: 500
        concurrency_level: 50
      - num_requests: 1000
        concurrency_level: 100
      - num_requests: 2000
        concurrency_level: 200
      - num_requests: 3000
        concurrency_level: 300
      - num_requests: 4000
        concurrency_level: 400
      num_workers: 45
      worker_max_concurrency: 10
    api:
      type: completion
      streaming: true
    server:
      type: vllm
      model_name: *model
      base_url: *url
      ignore_eos: true
    tokenizer:
      pretrained_model_name_or_path: *model
    data:
      type: random
      input_distribution:
        min: 1000
        max: 1000
        mean: 1000
        std: 0
      output_distribution:
        min: 1000
        max: 1000
        mean: 1000
        std: 0
    report:
      request_lifecycle:
        summary: true
        per_stage: true
        per_request: true
    storage:
      local_storage:
        path: /workspace
