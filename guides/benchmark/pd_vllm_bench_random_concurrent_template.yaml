endpoint:

  stack_name: &stack_name Qwen2-0.5B  # user defined name for the stack (results prefix)

  model: &model RedHatAI/Qwen2-0.5B-Instruct-FP8       # Exact HuggingFace model name. Must match stack deployed.

  namespace: &namespace ${NAMESPACE}  # Namespace where stack is deployed

  base_url: &url http://${GATEWAY_SVC}.${NAMESPACE}.svc.cluster.local:80  # Base URL of inference endpoint

  hf_token_secret: llm-d-hf-token     # The name of secret that contains the HF token of the stack


control:

  work_dir: $HOME/llm-d-bench-work  # working directory to store temporary and autogenerated files.

                                    # Do not edit content manually.

                                    # If not set, a temp directory will be created.

  kubectl: kubectl                  # kubectl command: kubectl or oc


harness:

  name: &harness_name vllm-benchmark

  results_pvc: ${BENCHMARK_PVC}   # PVC where benchmark results are stored

  namespace: *namespace           # Namespace where harness is deployed. Typically with stack.

  parallelism: 1                  # Number of parallel workload launcher pods to create.

  wait_timeout: 6000              # Time (in seconds) to wait for workload launcher pod to complete before terminating.

                                  # Set to 0 to disable timeout.

  image: ghcr.io/llm-d/llm-d-benchmark:v0.4.1

  # dataset_url: &dataset_url none


workload:                         # yaml configuration for harness workload(s)

  # an example of vllm benchmark workload using random synthetic data

  random_concurrent:

    executable: benchmark_serving.py

    model: *model

    base-url: *url

    dataset-name: random

    random-input-len: 10000

    random-output-len: 1000

    max-concurrency: 1

    num-prompts: 32

    percentile-metrics: "ttft,tpot,itl,e2el"

    metric-percentiles: "0.1,1,5,10,25,75,90,95,99,99.9"

    ignore-eos: none
