ARG CUDA_MAJOR=12
ARG CUDA_MINOR=9
ARG CUDA_PATCH=1

# ============================================================================
# BUILD STAGE - Install build dependencies and create wheels
# ============================================================================
FROM nvcr.io/nvidia/cuda:${CUDA_MAJOR}.${CUDA_MINOR}.${CUDA_PATCH}-devel-ubi9 AS builder

ARG TARGETPLATFORM
ARG CUDA_MAJOR=12
ARG CUDA_MINOR=9
ARG CUDA_PATCH=1
ARG PYTHON_VERSION

ARG USE_SCCACHE=true
ARG NVSHMEM_CUDA_ARCHITECTURES="90a;100"

COPY docker/scripts/common/setup-sccache.sh /usr/local/bin/setup-sccache
RUN chmod +x /usr/local/bin/setup-sccache

COPY docker/scripts/cuda/builder/install-sccache.sh /tmp/install-sccache.sh
RUN --mount=type=secret,id=aws_access_key_id \
    --mount=type=secret,id=aws_secret_access_key \
    chmod +x /tmp/install-sccache.sh && \
    /tmp/install-sccache.sh && \
    rm /tmp/install-sccache.sh

WORKDIR /workspace

# Create UV constraint files
COPY docker/build-constraints.txt /tmp/build-constraints.txt
COPY docker/constraints.txt /tmp/constraints.txt

ENV LANG=C.UTF-8 \
    LC_ALL=C.UTF-8 \
    UV_LINK_MODE=copy \
    TORCH_CUDA_ARCH_LIST="9.0a;10.0+PTX" \
    PYTHON_VERSION=${PYTHON_VERSION:-3.12} \
    UV_TORCH_BACKEND=${UV_TORCH_BACKEND:-cu${CUDA_MAJOR}${CUDA_MINOR}} \
    UV_BUILD_CONSTRAINT=/tmp/build-constraints.txt \
    UV_CONSTRAINT=/tmp/constraints.txt \
    VIRTUAL_ENV=/opt/vllm

# Update base packages
RUN dnf -q update -y && dnf clean all

# Install base packages and EPEL in single layer
COPY docker/scripts/cuda/builder/install-base-packages.sh /tmp/install-base-packages.sh
RUN chmod +x /tmp/install-base-packages.sh && \
    /tmp/install-base-packages.sh && \
    rm /tmp/install-base-packages.sh

# Setup Python virtual environment
RUN python${PYTHON_VERSION} -m venv /opt/vllm && \
    ${VIRTUAL_ENV}/bin/pip install --progress-bar off --no-cache -U pip wheel uv meson-python ninja pybind11 build

ENV LIBRARY_PATH="/usr/local/cuda/lib64:/usr/local/lib:/usr/local/lib64" \
    CPATH="/usr/include:/usr/local/include:/usr/local/cuda/include" \
    PKG_CONFIG_PATH="/usr/lib/pkgconfig:/usr/local/lib/pkgconfig:/usr/local/lib64/pkgconfig"

ARG NVSHMEM_VERSION=3.3.20

# Set NVSHMEM paths for CMake discovery
ENV NVSHMEM_DIR="/opt/nvshmem-${NVSHMEM_VERSION}" \
    PATH="/opt/nvshmem-${NVSHMEM_VERSION}/bin:${PATH}" \
    CPATH="/opt/nvshmem-${NVSHMEM_VERSION}/include:${CPATH}" \
    LIBRARY_PATH="/opt/nvshmem-${NVSHMEM_VERSION}/lib:${LIBRARY_PATH}"

# Build and install gdrcopy
COPY docker/scripts/cuda/builder/build-gdrcopy.sh /tmp/build-gdrcopy.sh
RUN --mount=type=cache,target=/var/cache/git \
    --mount=type=secret,id=aws_access_key_id \
    --mount=type=secret,id=aws_secret_access_key \
    chmod +x /tmp/build-gdrcopy.sh && \
    TARGETPLATFORM=${TARGETPLATFORM} /tmp/build-gdrcopy.sh && \
    rm /tmp/build-gdrcopy.sh

ENV CPPFLAGS="-I$NVSHMEM_DIR/include ${CPPFLAGS}" \
    LDFLAGS="-L$NVSHMEM_DIR/lib ${LDFLAGS}"

# Create wheels directory
RUN mkdir -p /wheels

# Copy patches before build
COPY patches/ /tmp/patches/

# Build and install NVSHMEM from source with coreweave patch (skip on ARM64)
COPY docker/scripts/cuda/builder/build-nvshmem.sh /tmp/build-nvshmem.sh
RUN --mount=type=secret,id=aws_access_key_id \
    --mount=type=secret,id=aws_secret_access_key \
    chmod +x /tmp/build-nvshmem.sh && \
    if [ "${TARGETPLATFORM}" != "linux/arm64" ]; then \
        NVSHMEM_CUDA_ARCHITECTURES="${NVSHMEM_CUDA_ARCHITECTURES}" /tmp/build-nvshmem.sh; \
    else \
        echo "Skipping NVSHMEM build on ARM64"; \
        mkdir -p "${NVSHMEM_DIR}"; \
    fi && \
    rm /tmp/build-nvshmem.sh

# Pin torch, so all deps are built against the same version
# as vllm itself
# hadolint ignore=SC1091
RUN --mount=type=cache,target=/root/.cache/uv \
  source ${VIRTUAL_ENV}/bin/activate && \
  uv pip install \
    # global
    numpy torch \
    pyyaml \
    types-PyYAML \
    pytest \
    patchelf>=0.11.0

RUN echo "/usr/local/lib" > /etc/ld.so.conf.d/local.conf && \
    echo "/usr/local/lib64" >> /etc/ld.so.conf.d/local.conf && \
    ldconfig

WORKDIR /workspace

# Define commit SHAs as build args to avoid layer invalidation
ARG LMCACHE_REPO=https://github.com/neuralmagic/LMCache.git
ARG LMCACHE_COMMIT_SHA="0db8ae4746a207a72483d716b5f82545a2ead24b"
ARG VLLM_COMMIT_SHA="b8b302cde434df8c9289a2b465406b47ebab1c2d"

# Build LMCache wheel (skip on ARM64 - infinistore dependency has no ARM wheels)
# hadolint ignore=DL3003,SC1091
RUN --mount=type=cache,target=/var/cache/git \
    if [ "${TARGETPLATFORM}" != "linux/arm64" ]; then \
        git clone "${LMCACHE_REPO}" LMCache && \
        cd LMCache && \
        git checkout -q "${LMCACHE_COMMIT_SHA}" && \
        cd .. && \
        cd LMCache && \
        source ${VIRTUAL_ENV}/bin/activate && \
        uv build --wheel --no-build-isolation --out-dir /wheels  && \
        cd ..; \
    else \
        echo "Skipping LMCache build on ARM64 (infinistore dependency lacks ARM wheels)"; \
    fi

# Use existing virtual environment at /opt/vllm
WORKDIR /workspace/vllm

# set kernel library dependencies
# note: these libraries don't yet push sdist releases to pypi
# so down below we do a git clone
ARG DEEPEP_REPO="https://github.com/deepseek-ai/DeepEP"
ARG DEEPEP_VERSION="v1.2.1"
ARG DEEPGEMM_REPO="https://github.com/deepseek-ai/DeepGEMM"
ARG DEEPGEMM_VERSION="v2.1.0"
ARG PPLX_KERNELS_REPO="https://github.com/perplexityai/pplx-kernels"
ARG PPLX_KERNELS_SHA="12cecfda252e4e646417ac263d96e994d476ee5d"

ARG FLASHINFER_VERSION="v0.3.1"

# Build compiled packages as wheels (only ones that need build tools)
COPY docker/scripts/cuda/builder/build-compiled-wheels.sh /tmp/build-compiled-wheels.sh
RUN --mount=type=cache,target=/root/.cache/uv \
    --mount=type=secret,id=aws_access_key_id \
    --mount=type=secret,id=aws_secret_access_key \
    chmod +x /tmp/build-compiled-wheels.sh && \
    TARGETPLATFORM=${TARGETPLATFORM} /tmp/build-compiled-wheels.sh && \
    rm /tmp/build-compiled-wheels.sh

# ============================================================================
# RUNTIME STAGE - Minimal runtime image
# ============================================================================
FROM nvcr.io/nvidia/cuda:${CUDA_MAJOR}.${CUDA_MINOR}.${CUDA_PATCH}-devel-ubi9 AS runtime

ARG CUDA_MAJOR=12
ARG CUDA_MINOR=9
ARG CUDA_PATCH=1
ARG PYTHON_VERSION
ARG NVSHMEM_VERSION=3.3.20

COPY docker/constraints.txt /tmp/constraints.txt

ENV LANG=C.UTF-8 \
    LC_ALL=C.UTF-8 \
    PYTHON_VERSION=${PYTHON_VERSION:-3.12} \
    UV_TORCH_BACKEND=${UV_TORCH_BACKEND:-cu${CUDA_MAJOR}${CUDA_MINOR}} \
    UV_CONSTRAINT=/tmp/constraints.txt \
    VIRTUAL_ENV=/opt/vllm \
    NVSHMEM_DIR="/opt/nvshmem-${NVSHMEM_VERSION}" \
    # LD_LIBRARY_PATH needs the torch path to apply proper linkers so as not to produce torch ABI missmatch
    LD_LIBRARY_PATH="/opt/vllm/lib64/python${PYTHON_VERSION}/site-packages/torch/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/cuda/lib64:/usr/local/lib:/usr/local/lib64:/opt/nvshmem-${NVSHMEM_VERSION}/lib:${LD_LIBRARY_PATH}" \
    PATH="/opt/nvshmem-${NVSHMEM_VERSION}/bin:${PATH}" \
    CPATH="/opt/nvshmem-${NVSHMEM_VERSION}/include:${CPATH}" \
    TORCH_CUDA_ARCH_LIST="9.0a;10.0+PTX"

# Update base packages
RUN dnf update -y && dnf clean all

# Install only runtime dependencies
# hadolint ignore=DL3041
RUN dnf install -y --allowerasing \
        python${PYTHON_VERSION} python${PYTHON_VERSION}-pip python${PYTHON_VERSION}-devel \
        rdma-core-devel \
        numactl-libs \
        pciutils \
        procps-ng \
        git \
        curl \
        jq \
        gcc \
        cuda-nvcc-${CUDA_MAJOR}-${CUDA_MINOR} && dnf clean all

# Copy gdrcopy libraries from builder
COPY --from=builder /usr/lib64/libgdrapi.so.2.* /usr/lib64/
COPY --from=builder /usr/local/lib/libgdrapi.so* /usr/local/lib/

# Copy compiled NVSHMEM libraries from builder
COPY --from=builder /opt/nvshmem-${NVSHMEM_VERSION}/ /opt/nvshmem-${NVSHMEM_VERSION}/

# Setup ldconfig and library paths
RUN echo "/usr/local/lib" > /etc/ld.so.conf.d/local.conf && \
    echo "/usr/local/lib64" >> /etc/ld.so.conf.d/local.conf && \
    echo "/opt/nvshmem-${NVSHMEM_VERSION}/lib" >> /etc/ld.so.conf.d/local.conf && \
    ldconfig

# Setup Python virtual environment
RUN python${PYTHON_VERSION} -m venv /opt/vllm && \
    ${VIRTUAL_ENV}/bin/pip install --no-cache -U pip wheel uv

# Copy compiled wheels
COPY --from=builder /wheels/*.whl /tmp/wheels/

# Create the vllm user
RUN useradd --uid 2000 --gid 0 vllm && \
    touch /home/vllm/.bashrc

# Create the vllm workspace with permissions to swap commits and remotes
# hadolint ignore=SC2015
RUN mkdir -p /opt/vllm-source && \
    chown -R 2000:0 /opt/vllm-source && \
    chmod -R g+rwX /opt/vllm-source && \
    find /opt/vllm-source -type d -exec chmod g+s {} \; && \
    setfacl -R -m g:0:rwX -m d:g:0:rwX /opt/vllm-source || true

# Define commit SHAs as build args to avoid layer invalidation
ARG LMCACHE_COMMIT_SHA="0db8ae4746a207a72483d716b5f82545a2ead24b"
ARG VLLM_REPO="https://github.com/vllm-project/vllm.git"
ARG VLLM_COMMIT_SHA="b8b302cde434df8c9289a2b465406b47ebab1c2d"
# vllm commit to use for precompiled wheel lookup (defaults to VLLM_COMMIT_SHA)
# useful for testing feature branches with python-only changes against merge-base binaries
ARG VLLM_PRECOMPILED_WHEEL_COMMIT

# Dictates if we should pull a production wheel for the vllm commit sha
ARG VLLM_PREBUILT=0
# Dictates if we should pull precompiled binaries when installing vllm editably. These commits must be on main in vLLM.
ARG VLLM_USE_PRECOMPILED=1

COPY scripts/warn-vllm-precompiled.sh /opt/

# Install cuda-python, nvshmem python bindings, xet
# Install all compiled wheels (DeepEP, DeepGEMM, pplx-kernels, LMCache, nixl)
# Installs vllm source. Supports three install modes:
    # 1) install vllm from source totally editably - dev option, supports fork and commit swapping
    # 3) install vllm from source with pulling precomiled binaries (shared libraries) from the vllm wheels index - less flexible dev option, may or may not work swapping shas but faster than full editable
    # 2) install vllm from a wheel on the vllm wheels index - prod option, no flexbility
COPY docker/scripts/cuda/runtime/install-vllm.sh /tmp/install-vllm.sh
RUN --mount=type=cache,target=/var/cache/git \
    chmod +x /tmp/install-vllm.sh && \
    /tmp/install-vllm.sh && \
    rm /tmp/install-vllm.sh

RUN dnf autoremove -y && dnf clean all

# setup non-root user for OpenShift
RUN umask 002 && \
    rm -rf /home/vllm && \
    mkdir -p /home/vllm && \
    chown vllm:root /home/vllm && \
    chmod g+rwx /home/vllm

# default openionated env var for HF_HOME, over-writeable
ENV LLM_D_MODELS_DIR=/var/lib/llm-d/models \
    HF_HOME=/var/lib/llm-d/.hf

# creates default models directory and makes path writeable for both root and default user, with symlink for convenience
# find command keeps group=0 on all new subdirs created later
RUN mkdir -p "$LLM_D_MODELS_DIR" "$HF_HOME" && \
    chown -R root:0 /var/lib/llm-d && \
    chmod -R g+rwX /var/lib/llm-d && \
    find /var/lib/llm-d -type d -exec chmod g+s {} \; && \
    ln -snf /var/lib/llm-d/models /models

ENV PATH="${VIRTUAL_ENV}/bin:/usr/local/nvidia/bin:${PATH}" \
    HOME=/home/vllm \
    VLLM_USAGE_SOURCE=production-docker-image \
    VLLM_WORKER_MULTIPROC_METHOD=fork \
    OUTLINES_CACHE_DIR=/tmp/outlines \
    NUMBA_CACHE_DIR=/tmp/numba \
    TRITON_CACHE_DIR=/tmp/triton \
    TRITON_LIBCUDA_PATH=/usr/lib64 \
    TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC=15 \
    TORCH_NCCL_DUMP_ON_TIMEOUT=0 \
    VLLM_SKIP_P2P_CHECK=1 \
    VLLM_CACHE_ROOT=/tmp/vllm \
    UCX_MEM_MMAP_HOOK_MODE=none

USER 2000
WORKDIR /home/vllm

ENTRYPOINT ["python", "-m", "vllm.entrypoints.openai.api_server"]
