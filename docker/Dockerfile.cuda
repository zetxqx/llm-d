ARG CUDA_MAJOR=12
ARG CUDA_MINOR=9
ARG CUDA_PATCH=1
ARG TARGETOS=rhel
ARG BASE_IMAGE_SUFFIX=ubi9

# Important: For Ubuntu builds, we use an older version (20.04) in the build stage
# to maintain broad glibc compatibility. Binaries built with newer glibc versions
# are not backwards compatible with OSes using earlier glibc versions.
# Runtime stage can use newer Ubuntu (22.04) for updated dependencies.
# See: https://github.com/vllm-project/vllm/blob/main/docker/Dockerfile#L18-L24
ARG BUILD_BASE_IMAGE_SUFFIX=${BASE_IMAGE_SUFFIX}
ARG FINAL_BASE_IMAGE_SUFFIX=${BASE_IMAGE_SUFFIX}

# ============================================================================
# BUILD STAGE - Install build dependencies and create wheels
# ============================================================================
FROM nvcr.io/nvidia/cuda:${CUDA_MAJOR}.${CUDA_MINOR}.${CUDA_PATCH}-devel-${BUILD_BASE_IMAGE_SUFFIX} AS builder

ARG CACHE_BUSTER
RUN if [ -n "${CACHE_BUSTER}" ]; then \
        echo "$CACHE_BUSTER" > /tmp/builder-buster; \
    fi;

ARG TARGETPLATFORM
ARG TARGETOS=rhel
ARG BUILD_BASE_IMAGE_SUFFIX=ubi9
ARG CUDA_MAJOR=12
ARG CUDA_MINOR=9
ARG CUDA_PATCH=1
ARG PYTHON_VERSION

ARG USE_SCCACHE=true
ARG NVSHMEM_CUDA_ARCHITECTURES="90a;100"

WORKDIR /workspace

# Create UV constraint files
COPY docker/build-constraints.txt /tmp/build-constraints.txt
COPY docker/constraints.txt /tmp/constraints.txt
COPY patches/ /tmp/patches/

ENV LANG=C.UTF-8 \
    LC_ALL=C.UTF-8 \
    UV_HTTP_TIMEOUT=500 \
    UV_INDEX_STRATEGY=unsafe-best-match \
    UV_LINK_MODE=copy \
    TORCH_CUDA_ARCH_LIST="9.0a;10.0+PTX" \
    PYTHON_VERSION=${PYTHON_VERSION:-3.12} \
    UV_TORCH_BACKEND=${UV_TORCH_BACKEND:-cu${CUDA_MAJOR}${CUDA_MINOR}} \
    UV_BUILD_CONSTRAINT=/tmp/build-constraints.txt \
    UV_CONSTRAINT=/tmp/constraints.txt \
    VIRTUAL_ENV=/opt/vllm

# Install base packages and EPEL in single layer
COPY docker/scripts/cuda/common/package-utils.sh /tmp/package-utils.sh
COPY docker/packages /tmp/packages
COPY docker/scripts/cuda/builder/install-base-packages.sh /tmp/install-base-packages.sh
RUN chmod +x /tmp/install-base-packages.sh && \
    TARGETOS=${TARGETOS} /tmp/install-base-packages.sh && \
    rm /tmp/install-base-packages.sh /tmp/package-utils.sh && \
    rm -rf /tmp/packages

# Install cmake from script for consistency between ubuntu22.04 and rhel9
COPY docker/scripts/cuda/builder/install-cmake.sh /tmp/install-cmake.sh
RUN --mount=type=cache,target=/var/cache/git \
    /tmp/install-cmake.sh && \
    rm -f /tmp/install-cmake.sh

# Install sccache (after base packages which include curl and openssl-devel)
COPY docker/scripts/common/setup-sccache.sh /usr/local/bin/setup-sccache
COPY docker/scripts/cuda/builder/install-sccache.sh /tmp/install-sccache.sh
RUN --mount=type=secret,id=aws_access_key_id \
    --mount=type=secret,id=aws_secret_access_key \
    chmod +x /usr/local/bin/setup-sccache /tmp/install-sccache.sh && \
    TARGETOS=${TARGETOS} /tmp/install-sccache.sh && \
    rm /tmp/install-sccache.sh

# Install uv and setup Python virtual environment
RUN export UV_INSTALL_DIR=/usr/local/bin && \
    curl -LsSf https://astral.sh/uv/install.sh | sh && \
    uv venv /opt/vllm --python ${PYTHON_VERSION} && \
    uv pip install --no-cache -U wheel meson-python ninja pybind11 build

ENV PATH="${VIRTUAL_ENV}/bin:${PATH}" \
    LIBRARY_PATH="/usr/local/cuda/lib64:/usr/local/lib:/usr/local/lib64" \
    CPATH="/usr/include:/usr/local/include:/usr/local/cuda/include" \
    PKG_CONFIG_PATH="/usr/lib/pkgconfig:/usr/local/lib/pkgconfig:/usr/local/lib64/pkgconfig"

# Create wheels directory
RUN mkdir -p /wheels

# Build and install gdrcopy
COPY docker/scripts/cuda/builder/build-gdrcopy.sh /tmp/build-gdrcopy.sh
RUN --mount=type=cache,target=/var/cache/git \
    --mount=type=secret,id=aws_access_key_id \
    --mount=type=secret,id=aws_secret_access_key \
    chmod +x /tmp/build-gdrcopy.sh && \
    TARGETPLATFORM=${TARGETPLATFORM} TARGETOS=${TARGETOS} /tmp/build-gdrcopy.sh && \
    rm /tmp/build-gdrcopy.sh

# Build and install NVSHMEM
ARG NVSHMEM_VERSION=3.3.20

# Set NVSHMEM paths for CMake discovery
ENV NVSHMEM_DIR="/opt/nvshmem-${NVSHMEM_VERSION}" \
    PATH="/opt/nvshmem-${NVSHMEM_VERSION}/bin:${PATH}" \
    CPATH="/opt/nvshmem-${NVSHMEM_VERSION}/include:${CPATH}" \
    LIBRARY_PATH="/opt/nvshmem-${NVSHMEM_VERSION}/lib:${LIBRARY_PATH}"

ENV CPPFLAGS="-I$NVSHMEM_DIR/include ${CPPFLAGS}" \
    LDFLAGS="-L$NVSHMEM_DIR/lib ${LDFLAGS}"

COPY docker/scripts/cuda/builder/build-nvshmem.sh /tmp/build-nvshmem.sh
RUN --mount=type=cache,target=/var/cache/git \
    --mount=type=secret,id=aws_access_key_id \
    --mount=type=secret,id=aws_secret_access_key \
    chmod +x /tmp/build-nvshmem.sh && \
    NVSHMEM_CUDA_ARCHITECTURES="${NVSHMEM_CUDA_ARCHITECTURES}" /tmp/build-nvshmem.sh && \
    rm /tmp/build-nvshmem.sh

# Pin torch, so all deps are built against the same version
# as vllm itself
# hadolint ignore=SC1091
RUN --mount=type=cache,target=/root/.cache/uv \
  . ${VIRTUAL_ENV}/bin/activate && \
  uv pip install \
    # global
    numpy torch \
    pyyaml \
    types-PyYAML \
    pytest \
    patchelf>=0.11.0

RUN echo "/usr/local/lib" > /etc/ld.so.conf.d/local.conf && \
    echo "/usr/local/lib64" >> /etc/ld.so.conf.d/local.conf && \
    ldconfig

WORKDIR /workspace

# Define commit SHAs as build args to avoid layer invalidation
ARG LMCACHE_REPO=https://github.com/neuralmagic/LMCache.git
ARG LMCACHE_COMMIT_SHA="0db8ae4746a207a72483d716b5f82545a2ead24b"
ARG VLLM_COMMIT_SHA="439368496db48d8f992ba8c606a0c0b1eebbfa69"

# Build LMCache wheel (skip on ARM64 - infinistore dependency has no ARM wheels)
# hadolint ignore=DL3003,SC1091
RUN --mount=type=cache,target=/var/cache/git \
    if [ "${TARGETPLATFORM}" != "linux/arm64" ]; then \
        git clone "${LMCACHE_REPO}" LMCache && \
        cd LMCache && \
        git checkout -q "${LMCACHE_COMMIT_SHA}" && \
        cd .. && \
        cd LMCache && \
        . ${VIRTUAL_ENV}/bin/activate && \
        uv build --wheel --no-build-isolation --out-dir /wheels  && \
        cd ..; \
    else \
        echo "Skipping LMCache build on ARM64 (infinistore dependency lacks ARM wheels)"; \
    fi

# Use existing virtual environment at /opt/vllm
WORKDIR /workspace/vllm

# set kernel library dependencies
# note: these libraries don't yet push sdist releases to pypi
# so down below we do a git clone
ARG DEEPEP_REPO="https://github.com/smarterclayton/DeepEP"
ARG DEEPEP_VERSION="nic_pe_alignment"
ARG DEEPGEMM_REPO="https://github.com/deepseek-ai/DeepGEMM"
ARG DEEPGEMM_VERSION="v2.1.0"
ARG PPLX_KERNELS_REPO="https://github.com/perplexityai/pplx-kernels"
ARG PPLX_KERNELS_SHA="12cecfda252e4e646417ac263d96e994d476ee5d"

ARG FLASHINFER_VERSION="v0.3.1"

# Build compiled packages as wheels (only ones that need build tools)
COPY docker/scripts/cuda/builder/build-compiled-wheels.sh /tmp/build-compiled-wheels.sh
RUN --mount=type=cache,target=/root/.cache/uv \
    --mount=type=secret,id=aws_access_key_id \
    --mount=type=secret,id=aws_secret_access_key \
    chmod +x /tmp/build-compiled-wheels.sh && \
    TARGETPLATFORM=${TARGETPLATFORM} /tmp/build-compiled-wheels.sh && \
    rm /tmp/build-compiled-wheels.sh

# ============================================================================
# RUNTIME STAGE - Minimal runtime image
# ============================================================================
FROM nvcr.io/nvidia/cuda:${CUDA_MAJOR}.${CUDA_MINOR}.${CUDA_PATCH}-devel-${FINAL_BASE_IMAGE_SUFFIX} AS runtime

ARG TARGETOS=rhel
ARG FINAL_BASE_IMAGE_SUFFIX=ubi9
ARG CUDA_MAJOR=12
ARG CUDA_MINOR=9
ARG CUDA_PATCH=1
ARG PYTHON_VERSION
ARG NVSHMEM_VERSION=3.3.20

COPY docker/constraints.txt /tmp/constraints.txt

ENV LANG=C.UTF-8 \
    LC_ALL=C.UTF-8 \
    PYTHON_VERSION=${PYTHON_VERSION:-3.12} \
    UV_TORCH_BACKEND=${UV_TORCH_BACKEND:-cu${CUDA_MAJOR}${CUDA_MINOR}} \
    UV_CONSTRAINT=/tmp/constraints.txt \
    VIRTUAL_ENV=/opt/vllm \
    NVSHMEM_DIR="/opt/nvshmem-${NVSHMEM_VERSION}" \
    TORCH_CUDA_ARCH_LIST="9.0a;10.0+PTX"

# LD_LIBRARY_PATH needs the torch path to apply proper linkers so as not to produce torch ABI missmatch
ENV LD_LIBRARY_PATH="/opt/vllm/lib64/python${PYTHON_VERSION}/site-packages/torch/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/cuda/lib64:/usr/local/lib:/usr/local/lib64:${NVSHMEM_DIR}/lib:${LD_LIBRARY_PATH}"

# Install runtime dependencies
COPY docker/scripts/cuda/common/package-utils.sh /tmp/package-utils.sh
COPY docker/packages /tmp/packages
COPY docker/scripts/cuda/runtime/install-runtime-packages.sh /tmp/install-runtime-packages.sh
RUN chmod +x /tmp/install-runtime-packages.sh && \
    TARGETOS=${TARGETOS} /tmp/install-runtime-packages.sh && \
    rm /tmp/install-runtime-packages.sh /tmp/package-utils.sh && \
    rm -rf /tmp/packages

# Copy gdrcopy libraries from builder
# Install to /usr/local/lib (always present) and multiarch lib dir
COPY --from=builder /usr/local/lib/libgdrapi.so* /usr/local/lib/
# Copy from builder's multiarch directory to runtime's matching directory
RUN if [ "${TARGETOS:-rhel}" = "ubuntu" ]; then \
        if [ "${TARGETPLATFORM:-linux/amd64}" = "linux/arm64" ]; then \
            LIBDIR="/usr/lib/aarch64-linux-gnu"; \
        else \
            LIBDIR="/usr/lib/x86_64-linux-gnu"; \
        fi; \
    else \
        LIBDIR="/usr/lib64"; \
    fi && \
    mkdir -p "${LIBDIR}"
COPY --from=builder /tmp/gdrcopy_libs /tmp/
RUN if [ -d /tmp/gdrcopy_libs ]; then \
        if [ "${TARGETOS:-rhel}" = "ubuntu" ]; then \
            if [ "${TARGETPLATFORM:-linux/amd64}" = "linux/arm64" ]; then \
                cp -a /tmp/gdrcopy_libs/* /usr/lib/aarch64-linux-gnu/ || true; \
            else \
                cp -a /tmp/gdrcopy_libs/* /usr/lib/x86_64-linux-gnu/ || true; \
            fi; \
        else \
            cp -a /tmp/gdrcopy_libs/* /usr/lib64/ || true; \
        fi; \
        rm -rf /tmp/gdrcopy_libs; \
    fi

# Copy NVSHMEM dir from builder
COPY --from=builder /opt/nvshmem-${NVSHMEM_VERSION}/ /opt/nvshmem-${NVSHMEM_VERSION}/

# Setup ldconfig and library paths
RUN echo "/usr/local/lib" > /etc/ld.so.conf.d/local.conf && \
    echo "/usr/local/lib64" >> /etc/ld.so.conf.d/local.conf && \
    ldconfig

# Install uv and setup Python virtual environment
RUN export UV_INSTALL_DIR=/usr/local/bin && \
    curl -LsSf https://astral.sh/uv/install.sh | sh && \
    uv venv /opt/vllm --python ${PYTHON_VERSION} && \
    rm -f /usr/bin/python3 /usr/bin/python3-config /usr/bin/pip && \
    ln -s /opt/vllm/bin/python3 /usr/bin/python3 && \
    ln -s /opt/vllm/bin/python3-config /usr/bin/python3-config && \
    ln -s /opt/vllm/bin/pip /usr/bin/pip && \
    uv pip install --no-cache -U wheel

# Copy compiled wheels
COPY --from=builder /wheels/*.whl /tmp/wheels/

# Create the vllm user
RUN useradd --uid 2000 --gid 0 -m vllm && \
    touch /home/vllm/.bashrc

# Create the vllm workspace with permissions to swap commits and remotes
# hadolint ignore=SC2015
RUN mkdir -p /opt/vllm-source && \
    chown -R 2000:0 /opt/vllm-source && \
    chmod -R g+rwX /opt/vllm-source && \
    find /opt/vllm-source -type d -exec chmod g+s {} \; && \
    setfacl -R -m g:0:rwX -m d:g:0:rwX /opt/vllm-source || true

# Define commit SHAs as build args to avoid layer invalidation
ARG LMCACHE_COMMIT_SHA="0db8ae4746a207a72483d716b5f82545a2ead24b"
ARG VLLM_REPO="https://github.com/vllm-project/vllm.git"
ARG VLLM_COMMIT_SHA="439368496db48d8f992ba8c606a0c0b1eebbfa69"
# vllm commit to use for precompiled wheel lookup (defaults to VLLM_COMMIT_SHA)
# useful for testing feature branches with python-only changes against merge-base binaries
ARG VLLM_PRECOMPILED_WHEEL_COMMIT

# Dictates if we should pull a production wheel for the vllm commit sha
ARG VLLM_PREBUILT=0
# Dictates if we should pull precompiled binaries when installing vllm editably. These commits must be on main in vLLM.
ARG VLLM_USE_PRECOMPILED=1

COPY scripts/warn-vllm-precompiled.sh /opt/

# Install cuda-python, nvshmem python bindings, xet
# Install all compiled wheels (DeepEP, DeepGEMM, pplx-kernels, LMCache, nixl)
# Installs vllm source. Supports three install modes:
    # 1) install vllm from source totally editably - dev option, supports fork and commit swapping
    # 3) install vllm from source with pulling precomiled binaries (shared libraries) from the vllm wheels index - less flexible dev option, may or may not work swapping shas but faster than full editable
    # 2) install vllm from a wheel on the vllm wheels index - prod option, no flexbility
COPY docker/scripts/cuda/runtime/install-vllm.sh /tmp/install-vllm.sh
RUN --mount=type=cache,target=/var/cache/git \
    chmod +x /tmp/install-vllm.sh && \
    /tmp/install-vllm.sh && \
    rm /tmp/install-vllm.sh

# Cleanup packages
COPY docker/scripts/cuda/common/package-utils.sh /tmp/package-utils.sh
RUN bash -c "chmod +x /tmp/package-utils.sh && \
    # shellcheck source=docker/scripts/cuda/common/package-utils.sh
    . /tmp/package-utils.sh && \
    autoremove_packages \${TARGETOS} && \
    cleanup_packages \${TARGETOS} && \
    rm /tmp/package-utils.sh"

# setup non-root user for OpenShift
RUN umask 002 && \
    rm -rf /home/vllm && \
    mkdir -p /home/vllm && \
    chown vllm:root /home/vllm && \
    chmod g+rwx /home/vllm

# default opinionated env var for HF_HOME, over-writeable
ENV LLM_D_MODELS_DIR=/var/lib/llm-d/models \
    HF_HOME=/var/lib/llm-d/.hf

# creates default models directory and makes path writeable for both root and default user, with symlink for convenience
# find command keeps group=0 on all new subdirs created later
RUN mkdir -p "$LLM_D_MODELS_DIR" "$HF_HOME" && \
    chown -R root:0 /var/lib/llm-d && \
    chmod -R g+rwX /var/lib/llm-d && \
    find /var/lib/llm-d -type d -exec chmod g+s {} \; && \
    ln -snf /var/lib/llm-d/models /models

ENV PATH="${VIRTUAL_ENV}/bin:/usr/local/nvidia/bin:${PATH}" \
    HOME=/home/vllm \
    VLLM_USAGE_SOURCE=production-docker-image \
    VLLM_WORKER_MULTIPROC_METHOD=fork \
    OUTLINES_CACHE_DIR=/tmp/outlines \
    NUMBA_CACHE_DIR=/tmp/numba \
    TRITON_CACHE_DIR=/tmp/triton \
    # TRITON_LIBCUDA_PATH: On RHEL use /usr/lib64, on Ubuntu ARM64 use /usr/lib/aarch64-linux-gnu, on Ubuntu AMD64 use /usr/lib/x86_64-linux-gnu
    # Can be overridden at runtime via deployment config
    TRITON_LIBCUDA_PATH=/usr/lib64 \
    TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC=15 \
    TORCH_NCCL_DUMP_ON_TIMEOUT=0 \
    VLLM_SKIP_P2P_CHECK=1 \
    VLLM_CACHE_ROOT=/tmp/vllm \
    UCX_MEM_MMAP_HOOK_MODE=none

USER 2000
WORKDIR /home/vllm

ENTRYPOINT ["python", "-m", "vllm.entrypoints.openai.api_server"]
# test
